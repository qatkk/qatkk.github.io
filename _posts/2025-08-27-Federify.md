---
title: 'Federify: Making Federated Learning Verifiable and Secure'
date: 2025-08-27
permalink: /posts/2025/08/Federify/
tags:
  - ZKSNARKs
  - Federated Learning
  - Security
---
In this post, I explain the need for verification in Federated learning and how we suggest to do it in our paper Federify.

Federify: Making Federated Learning Verifiable and Secure
=============
**By Ghazaleh Keshavarzkalhori, Cristina Perez-Sola, Guillermo Navarro-Arribas, Jordi Herrera-JoancomartÃ­, and Habib Yajam**


In a world where data privacy is more important than ever, **federated learning (FL)** has emerged as a powerful way to train machine learning models collaborativelyâ€”without ever needing to share raw data. Originally introduced by Google, this approach allows data owners to train models locally on their data, and then send those trained models (not the data) back to a central server where they are aggregated into a global model.

![A diagram of a computer network](/images/posts/Federify/1.png)

Itâ€™s an elegant solutionâ€”until you start looking at the **privacy and security challenges** under the hood.


## Why Federated Learning Isnâ€™t Enough for Privacy?

Federated learning was initially praised for its privacy benefits. For instance, Google used it to train the Gboard keyboard while keeping usersâ€™ texts private.  

But if all models are sent to a server to be combined, someone could try typing in half-finished sentences and see how the model responds. That could reveal how people usually writeâ€”a serious **privacy issue**.

### Common Attacks in Federated Learning:
- **Membership inference attacks** â†’ Detect if a specific userâ€™s data was used to train the model.  
- **Property inference attacks** â†’ Guess hidden details (e.g., location, device).  
- **Reconstruction attacks** â†’ Attempt to recreate the original data.  

In short: **distributed training introduces trust issues**. Can we trust:
1. Other participants not to send false data (poisoning)?
2. The server to correctly and fairly compute the final model?

The answer is often **no**.


## Introducing Federify: A New Approach

To address these issues, we propose **Federify**, a verifiable federated learning scheme that combines:
- **zkSNARKs**
- **Blockchain**
- **Homomorphic encryption**

ðŸ”— [Proof-of-concept on GitHub: FederatedNaiveBayesZKP](https://github.com/)  



### 1. Unfair Central Server â†’ Blockchain & Smart Contracts

Instead of relying on a traditional server, **blockchain smart contracts** provide transparency and automation.  
This ensures verifiable model aggregation without needing to trust a central server.  

![Diagram of blockchain technology](/images/posts/Federify/2.png)



### 2. Untrusted Data Owners â†’ Zero-Knowledge Proofs

Zero-knowledge proofs (ZKPs) allow participants to prove a computation was done correctly **without revealing sensitive inputs**.  

In Federify:
- Each data owner trains their model locally.  
- They generate a zkSNARK proof of correct training.  
- The smart contract only accepts valid models with verified proofs.  

[Learn more about zkSNARKs](https://z.cash/technology/zksnarks/)  


### 3. Information Leakage Through Models â†’ Homomorphic Encryption

Sharing local models publicly risks **information leakage**.  
Federify solves this with **homomorphic encryption**, which allows computations on encrypted data without decrypting it.  

- Models remain hidden.  
- Only collaborative decryption is possible (no single participant can decrypt).  

![Diagram of homomorphic encryption](/images/posts/Federify/3.png)


## Summary of Federifyâ€™s Protections

- **Smart contracts** â†’ Transparent aggregation, no unfair servers.  
- **Homomorphic encryption** â†’ Model privacy preserved.  
- **zkSNARKs** â†’ Verifiable training, no need to trust participants.  

---

ðŸ“„ For further technical details, read the full paper:  
[**Federify: A Verifiable Federated Learning Scheme Based on zkSNARKs and Blockchain**](https://ieeexplore.ieee.org/abstract/document/10373785)
